{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2a78c1976a864f8e80c0e4d75c0ab8d4",
      "5427a72c723f4cae82aa37862151f7a3",
      "903d145c69054db6a2b16598ff3bf383",
      "acd40c9e6fb44b619aafbef43bc7ae1d",
      "e7f32cd43fb54e1982a09a8a08998eb0",
      "59f358bb30e145a28c200a5ea9f62c70",
      "a679272125a045b2a7f43ac0f42ceec9",
      "d6e519d1f8514079bd892c58768a95ea",
      "cf8cc0b97aaf4320803401d2e9c64128",
      "2d5dc712887b4d01ae4a6522a8c16756",
      "dbf3a5425995492593f8f32a67fe23f8",
      "04a00d14740f421c9a15bc4bb4c2f802",
      "f20b8056e6504f50ba8a05b643eebf6b",
      "2709ba11d5644f4f873e3624b9d8363f",
      "282139551b4b4fe5a0f00f95c9173aa0",
      "ce94289d7c0f43a595d7a8695fd6335c",
      "6b38e1a0685342d8a2e2ce9f1018f24c",
      "911571fd63d1475589a6799cd4801155",
      "f982e568cae6428d89dfae10588c6d65",
      "c7104048cfa546ff8953a6007f139f83",
      "82bbc3bcb4ba4d129287dba416de8454",
      "92cd1c4e43ae4bee94524446e542b69e",
      "d4f62bb10fb3406a9184ca3ca7a06adf",
      "3d0de7d6ebed449baf1b0c8c6910d7a1",
      "cd9047c65e244320a5cc5e3f997761da",
      "57116941ba134933a1298a6175f10f70",
      "153deaf11f6f4fd79fadc1ae16fe57ac",
      "936dc50a75954b7a9af20ff9c7ec13c1",
      "f3b3723502354754a3b7000de3c0ba7b",
      "076f2890cce043b8bd0d53ca81faee92",
      "1218405432ed442facf79b57d3ef84ae",
      "acb9591666cc46d688d767320e4992c6"
     ]
    },
    "id": "qZRM3Huxoue1",
    "outputId": "731409d2-8e5d-44b8-a32a-2b5fd16c27cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db0d9f36cb44339a9b6f879aa487144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HUGGINGFACE_TOKEN = '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'\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qnQo2tc6oue3"
   },
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAXPGo5Eoue3"
   },
   "source": [
    "##### run in terminal in spread-me-not (after pipenv shell)\n",
    "\n",
    "##### huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eF8pc-vkoue4"
   },
   "outputs": [],
   "source": [
    "training_samples = [\n",
    "{'input': '@imbernomics @tyleransom @Noahpinion When teaching trade, I use higher education as an example of America\\'s greatest export...this policy will have long run implications.',\n",
    " 'output': {'atomic_facts': ['Higher education is used as an example of America\\'s greatest export in teaching trade.', 'The policy will have long-run implications.'],\n",
    "            'claim_type': 'opinion',\n",
    "            'sentiment': 'neutral'}},\n",
    "{'input': '\"Rana Foroohar Bubbles are bursting everywhere and America\\'s most prestigious export higher education won\\'t be immune...Universities are like landlocked cruise ships: places with all-you-can-eat buffets and plenty of beer, but almost no way of social distancing.\"',\n",
    " 'output': {'atomic_facts': ['Bubbles are bursting everywhere.', 'America\\'s most prestigious export higher education won\\'t be immune.', 'Universities are like landlocked cruise ships with limited social distancing.'],\n",
    "            'claim_type': 'opinion',\n",
    "            'sentiment': 'neutral'}},\n",
    "{\n",
    "  \"input\": \"\\\"Rana Foroohar Bubbles are bursting everywhere and Americas most prestigious export higher education wont be immune. Universities are like landlocked cruise ships: places with all-you-can-eat buffets and plenty of beer, but almost no way of social distancing.\\\"\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Bubbles are bursting everywhere.\", \"America's most prestigious export is higher education.\", \"Universities are like landlocked cruise ships with limited social distancing.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"Higher education is probably America's last known good export, after baywatch of course. The future of Higher Education in America may very well issue in the rise of America's next best export: Knowledge\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Higher education is probably America's last known good export.\", \"The future of Higher Education in America may result in the rise of America's next best export: Knowledge.\"],\n",
    "    \"claim_type\": \"opinion\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@dbessner Im increasingly coming around to the idea that Americas higher education system is not only a major export, but a critical component of American hegemony as well.\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"America's higher education system is not only a major export.\", \"It is a critical component of American hegemony.\"],\n",
    "    \"claim_type\": \"opinion\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"\\\"Bubbles are bursting everywhere and Americas most prestigious export higher education wont be immune. Universities are like landlocked cruise ships: places with all-you-can-eat buffets and plenty of beer, but almost no way of social distancing. @FinancialTimes\\\"\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Bubbles are bursting everywhere.\", \"America's most prestigious export is higher education.\", \"Universities are like landlocked cruise ships with limited social distancing.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"Higher education is America's greatest export industry. #gc2013 @BrazenlyVirile Believe it or not, America's best perform export is higher education. #TMBC @MaverickIndeed @OG_Humble_One\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Higher education is America's greatest export industry.\", \"America's best-performing export is higher education.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@HowieHawkins , I understand there was a time in the early 20th century when America's most revered export was higher education back when it was synonymous with the idea of being the great equalizer. A society that has devolved to everyone &amp; everything as a commodity is barbarism period.\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"In the early 20th century, America's most revered export was higher education.\", \"It was synonymous with the idea of being the great equalizer.\", \"The society has devolved to everyone and everything as a commodity, which is considered barbarism.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"Its likely that COVID is going to lead to devastating cuts in higher education. It be great if policy makers would recognize that higher education is one of Americas great assets and our most popular and successful export.\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"COVID is likely to lead to devastating cuts in higher education.\", \"Higher education is one of America's great assets.\", \"It is one of America's most popular and successful exports.\"],\n",
    "    \"claim_type\": \"opinion\",\n",
    "    \"sentiment\": \"sad\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@Istanbultelaviv I thought higher education was Americas largest export. Export because international students here bring in foreign money. Dont know if its largest but certainly a substantial part of our export economy. Whoevers pulling Trumps strings is doing good job of destroying USA.\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Higher education is thought to be America's largest export.\", \"International students bring in foreign money, contributing to the export economy.\", \"Someone is doing a good job of destroying the USA, possibly by pulling Trump's strings.\"],\n",
    "    \"claim_type\": \"opinion\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@FrankLuntz HIGHER EDUCATION IS ONE OF AMERICA'S MOST SUCCESSFUL EXPORT PRODUCTS, bringing billions of $s into our country each year Warren: Last year 36 million Americans didnt have a prescription filled because they couldnt afford it. Im going to attack the prices on commonly used drugs like EpiPens. #DemocraticDebate #DemDebate #PresidentialDebate #HealthCare\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Higher education is one of America's most successful export products, bringing billions of dollars into the country each year.\", \"Last year, 36 million Americans didn't have a prescription filled due to affordability issues.\", \"Warren plans to attack the prices on commonly used drugs like EpiPens.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"neutral\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"\\\"36 million Americans couldn't afford to get a prescription filled last year - and that includes people with insurance.\\\" @ewarren #DemDebate We need #MedicareForAll\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"36 million Americans couldn't afford to get a prescription filled last year, including people with insurance.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"sad\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"In 2018, 36 million Americans couldn't afford to have a prescription filled. We need to bring the most help to the most people as quickly as possible. We need #MedicareForAll. #DemDebate\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"In 2018, 36 million Americans couldn't afford to have a prescription filled.\", \"There is a need to bring the most help to the most people as quickly as possible.\", \"Advocacy for #MedicareForAll.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"sad\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"From #DemDebate: \\\"I started this by talking about 36 million americans including americans with insurance who can't afford to have a prescription filled.\\\" @SoSofieFatale @La_Ketch While 36 million Americans did not have a prescription filled last year because they couldnt afford it.\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"The #DemDebate discussed 36 million Americans, including those with insurance, who can't afford to have a prescription filled.\", \"36 million Americans did not have a prescription filled last year due to affordability issues.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"sad\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"Last year 36 million Americans didnt have a prescription filled because they couldnt afford it. Im going to attack the prices on commonly used drugs like EpiPens. - @ewarren #AFTvotes #DemDebate\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Last year, 36 million Americans didn't have a prescription filled due to affordability issues.\", \"Warren plans to attack the prices on commonly used drugs like EpiPens.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"sad\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@DrJasonJohnson part 2. If youre a black woman, youre 320 percent more likely to die from complications in childbirth. These are the numbers that define race in our country. He was right.But you dont talk about the substance of what he says. #YangGang\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"If you're a black woman, you're 320 percent more likely to die from complications in childbirth.\", \"These numbers define race in our country.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@GrundelplithMD @WSJ Yang (continued): \\\"The average net worth of a black household is only 10% that of a white household. For Latinos, its 12%. If youre a black woman, youre 320% more likely to die from complications in childbirth. These are the numbers that define race in our country.\\\"\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"The average net worth of a black household is only 10% that of a white household.\", \"For Latinos, it's 12%.\", \"If you're a black woman, you're 320% more likely to die from complications in childbirth.\", \"These numbers define race in our country.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"\\\"@ThisJamesCoker \\\"\\\"If you're a black woman, you're 320% more likely to die from complications in childbirth.\\\"\\\" -Andrew Yang, We need more politicians asserting these heartbreaking facts.\\\"\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"If you're a black woman, you're 320% more likely to die from complications in childbirth.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"If youre a black woman, youre 320% more likely to die from complications in #childbirth, @AndrewYang said in a discussion about racial disparities between black and white Americans. #maternalhealth #DemocraticDebate\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"If you're a black woman, you're 320% more likely to die from complications in childbirth.\", \"Discussion about racial disparities between black and white Americans.\", \"Maternal health is mentioned.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"If youre a black woman you have a 320% higher chance of dying in childbirth of all the horrible issues black women face, CHILDBIRTH is what you pick, Yang? What about rape? What about the pay gap? HELLO??? #DemDebate\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"If you're a black woman, you have a 320% higher chance of dying in childbirth.\", \"Childbirth is emphasized among the issues black women face.\", \"Rape and the pay gap are mentioned.\"],\n",
    "    \"claim_type\": \"opinion\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"@DrJasonJohnson 2 the average net worth of a black household is only 10 % that of a white household. For Latinos, its 12 %If youre a black woman, youre 320 % more likely to die from complications in childbirth. These are the numbers that define race in our country. #YangGang\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"The average net worth of a black household is only 10% that of a white household.\", \"For Latinos, it's 12%.\", \"If you're a black woman, you're 320% more likely to die from complications in childbirth.\", \"These numbers define race in our country.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"\\\"#BlacksForYang this should be concerning: The crazy thing is that this isnt new information yet its still a problem @BlacksForYang\\\" \\\"Not to be bothered with such facts the spin *will* continue...\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"#BlacksForYang expresses concern about concerning information.\", \"The information is not new but still a problem.\", \"There is an expectation of continued spin.\"],\n",
    "    \"claim_type\": \"opinion\",\n",
    "    \"sentiment\": \"angry\"\n",
    "  }\n",
    "},\n",
    "{\n",
    "  \"input\": \"\\\"BREAKING: Senate passes USMCA, giving Trump back-to-back trade wins! The USMCA is expected to increase annual U.S. agricultural and food exports by $2.2 billion. President Trump took action to help farmers while holding China accountable. What a difference a president makes!\\\"\",\n",
    "  \"output\": {\n",
    "    \"atomic_facts\": [\"Senate passes USMCA, giving Trump back-to-back trade wins.\", \"USMCA is expected to increase annual U.S. agricultural and food exports by $2.2 billion.\", \"President Trump took action to help farmers and hold China accountable.\"],\n",
    "    \"claim_type\": \"claim\",\n",
    "    \"sentiment\": \"happy\"\n",
    "  }\n",
    "},\n",
    "  {\n",
    "    \"input\": \"For every dollar spent on market development, the value of U.S. food and agricultural exports increase by $35. #AgDay\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"For every dollar spent on market development\", \"the value of U.S. food and agricultural exports increase by $35\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"In the next 40 minutes, will @CNN ask the candidates why they oppose #USMCA, knowing it will increase U.S. agricultural and food exports by at least $4 BILLION?\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"will @CNN ask the candidates why they oppose #USMCA\", \"knowing it will increase U.S. agricultural and food exports by at least $4 BILLION\"],\n",
    "      \"claim_type\": \"question\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"For every dollar spent on market development, U.S. food and agricultural exports increase by $35. #agday\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"For every dollar spent on market development\", \"U.S. food and agricultural exports increase by $35\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"RT @uswheatassoc: For every dollar spent on market development, the value of U.S. food and agricultural exports increase by $35. #AgDay\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"For every dollar spent on market development\", \"the value of U.S. food and agricultural exports increase by $35\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"There are currently 390 million guns in America, outnumbering the 329 million people in this country. Partly due to the ease with which individuals can access guns, America saw nearly 40,000 gun deaths in 2017. America cannot afford to wait for a new president, or for Congress,\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"There are currently 390 million guns in America\", \"outnumbering the 329 million people in this country\", \"Partly due to the ease with which individuals can access guns\", \"America saw nearly 40,000 gun deaths in 2017\", \"America cannot afford to wait for a new president, or for Congress\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"sad\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@Dukecolt_45 @Bobek996 @seanhannity There are more guns than people in the United States (400 million are in circulation for a population of 330 million). Dam you can't even comprehend your own facts, thats in CIRCULATION. There are 390 million guns out on the streets of a country of 329 million people.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"There are more guns than people in the United States (400 million are in circulation for a population of 330 million)\", \"There are 390 million guns out on the streets of a country of 329 million people\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@SenRonJohnson \\\"There are 390 million guns out on the streets of a country of 329 million people. (fact checked Beto) How about working on this? Stop gas-lighting us! Stop defending Trompudo! Do your job! It is not about the 2nd Amendment, and you know it! #GunReformNow\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"There are 390 million guns out on the streets of a country of 329 million people\", \"(fact checked Beto) How about working on this?\", \"Stop gas-lighting us!\", \"Stop defending Trompudo!\", \"It is not about the 2nd Amendment, and you know it!\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"\\\"@ltgrusselhonore @TheJusticeDept @NatlGovsAssoc @MSNBC @POTUS @VP @nytimes We cannot control the guns 390 million guns out on the streets of a country of 329 million people #1 DO NOT show the face or NAME of the shooters All Media must agree #2 All Mass shooters get Life in Prison solitary confinement #3 Police follow the law &amp; not their politics\\\"\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"We cannot control the guns 390 million guns out on the streets of a country of 329 million people\", \"#1 DO NOT show the face or NAME of the shooters\", \"All Media must agree\", \"#2 All Mass shooters get Life in Prison solitary confinement\", \"#3 Police follow the law & not their politics\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@RunningLogans @backdraft1212 @ATFDallas @DFWscanner @WSPD_TX @cityof_ws @fortworthpd 2/ The real world as you phrased it is full of law-abiding gun owners. People hunt, sport shoot all over the country and 390 million guns in a population of 329 million tells me my world is much more real than yours. In 2016, surveys noted Households with a Gun at 36-49%.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"The real world is full of law-abiding gun owners\", \"People hunt, sport shoot all over the country\", \"390 million guns in a population of 329 million tells me my world is much more real than yours\", \"In 2016, surveys noted Households with a Gun at 36-49%\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@JeffreyCook @ABCPolitics ***18 yrs old is the Legal Age to purchase an Assault Weapon. ***21 yrs old is the Legal Age to purchase a Hand Gun. 390 Million Guns in America 329 Million People in America More Guns have not made us more safe. #BetoForGunLawReform #Beto2020\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"18 yrs old is the Legal Age to purchase an Assault Weapon\", \"21 yrs old is the Legal Age to purchase a Hand Gun\", \"390 Million Guns in America\", \"329 Million People in America\", \"More Guns have not made us more safe\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@sahluwal \\\"There are 390 million guns out on the streets of a country of 329 million people. WHAT ABOUT #GunControlNow ? ASKING FOR 329 MILLION PEOPLE! @SenRonJohnson @js_politics\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"There are 390 million guns out on the streets of a country of 329 million people\", \"WHAT ABOUT #GunControlNow ?\", \"ASKING FOR 329 MILLION PEOPLE!\"],\n",
    "      \"claim_type\": \"question\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"\\\"@trustingmyvibes US population, 329.5 million people. 32% of that population owns at least 390 million guns. Mental health, domestic violence, racism/bigotry/misogyny &amp; more lead to #Gunviolence But the root of all this is the glorification of #gunCulture. Guns are THE PROBLEM #gunControlNow\\\"\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"US population, 329.5 million people\", \"32% of that population owns at least 390 million guns\", \"Mental health, domestic violence, racism/bigotry/misogyny & more lead to #Gunviolence\", \"The root of all this is the glorification of #gunCulture\", \"Guns are THE PROBLEM #gunControlNow\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Gillibrand just said she ran on Medicare for All when she ran for Senate in 2005. How did that work out?\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Gillibrand just said she ran on Medicare for All when she ran for Senate in 2005\"],\n",
    "      \"claim_type\": \"question\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"\\\"@SenGillibrand you ran on getting troops out of Iraq in 2005 and Medicare for all with 4-5% of income? Last I checked troops are still in Iraq in 2019 and that % is still too high unless incomes go up a lot. So I won't vote for you, unless... @alfranken is your running mate.\\\"\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"you ran on getting troops out of Iraq in 2005 and Medicare for all with 4-5% of income\", \"troops are still in Iraq in 2019\", \"that % is still too high unless incomes go up a lot\", \"So I won't vote for you, unless... @alfranken is your running mate\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"never forget, Matt Santos ran on Medicare for All in 2005, and won the Presidency. thats my President.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Matt Santos ran on Medicare for All in 2005\", \"won the Presidency\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"happy\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@SenGillibrand: Ran on Medicare for All in 2005. \\\"And I won.\\\" People will choose Medicare and \\\"your step to single payer is so short.\\\" #DemDebate2\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Ran on Medicare for All in 2005\", \"\\\"And I won\\\"\", \"People will choose Medicare\", \"\\\"your step to single payer is so short\\\"\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"happy\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@jamestyancey @paulkrugman Before anyone else? John Conyers intro'd Medicare for All to the house in 2003 and every year since. Kirstan Gillibrand ran on it in 2005/6. Bernie likes to steal others' ideas and pretend they are his.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"John Conyers intro'd Medicare for All to the house in 2003 and every year since\", \"Kirstan Gillibrand ran on it in 2005/6\", \"Bernie likes to steal others' ideas and pretend they are his\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@AdyBarkan @kielmw @MeghanMcCain @SenJohnMcCain My mom died of ALS in 2005 we had to FIGHT Medicare to keep her in a nursing home-when it ran out she was going to be PUT OUT. She couldn't SPEAK,EAT, SWALLOW, SPEAK. Now I'm on Medicare=disabled. If they take our tiny amt I recv my child & I'd NEVER SURVIVE-Help @SenJohnMcCain\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"My mom died of ALS in 2005\", \"FIGHT Medicare to keep her in a nursing home\", \"when it ran out she was going to be PUT OUT\", \"She couldn't SPEAK,EAT, SWALLOW, SPEAK\", \"Now I'm on Medicare=disabled\", \"If they take our tiny amt I recv my child & I'd NEVER SURVIVE-Help @SenJohnMcCain\"],\n",
    "      \"claim_type\": \"feeling\",\n",
    "      \"sentiment\": \"sad\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@ryanlcooper He was one of the first guys to introduce a climate change bill, way, way back in 87. It's not our fault you DID ignore his career, weirdo.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"He was one of the first guys to introduce a climate change bill, way, way back in 87\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"LIVE: American people, please take a look what a real election is, now is undergoing in Taiwan: paper ballots, manual counting, counting completed on the same day, and election results announced on the same day!\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"There is a current election happening in Taiwan.\", \"The election process in Taiwan involves the use of paper ballots.\", \"The counting of votes in Taiwan is done manually.\", \"The counting of votes in Taiwan is completed on the same day as the election.\", \"The election results in Taiwan are announced on the same day as the election.\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Pongal gifts were a disgrace to your failure followed by Michaung Cyclone relief fund 6k everything was literally cheated and false promises to people of TN. NO DMK NO Dravidian\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"The Pongal gifts provided were considered disgraceful and attributed to a failure.\", \"Allegation that the relief fund for Michaung Cyclone was mishandled, claiming everything related to the fund of 6,000 units (presumably currency) was cheated.\", \"Accusation that false promises were made to the people of Tamil Nadu.\", \"Clear opposition to DMK (Dravida Munnetra Kazhagam) and the Dravidian political ideology.\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Only ISRAEL would have to defend itself in the International Court Of Jerkoffs while 133 Hostages are still in captivity. TFOH!!! Day 98! Free The Hostages! #israel #South_Africa 🤡🤠 Why isn’t HAMAS in court?\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Only Israel has to defend itself in the International Court.\", \"Implication that 133 hostages are still in captivity.\", \"User questions why Hamas, a Palestinian militant group, is not facing legal consequences in the International Court.\", \"User urges for the release of the hostages.\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Useless collectors of #Chennai & #Kancheepram districts. Can't u see how it's pouring 💦.? Why can't u declare holiday for schools atleast? Especially OMR road is flooded with rain water & heavy traffic ⛔ This govt is a bunch of clowns seriously. !!😏🤷\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"It's pouring in Chennai & Kancheepuram districts.\", \"Expresses dissatisfaction with the performance of the district collectors.\", \"Call for declaring a holiday for schools due to heavy rain.\", \"The statement highlights the flooding of OMR road with rainwater and heavy traffic.\", \"Expresses the opinion that the government is a bunch of clowns.\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"It is funny of Hamas to expect global sympathy after beheading babies and raping women from israel\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Hamas expects global sympathy.\", \"Hamas has beheaded babies from Israel.\", \"Hamas raped women from Israel.\", \"The speaker believes that it is humorous or ironic.\"],\n",
    "      \"claim_type\": \"opinion\",\n",
    "      \"sentiment\": \"angry\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@Riguy_453 @PaulSorrentino3 @POTUS Trump didn't attempt to mandate mask!! Biden wanted to more than the vaccine but you can't get Republican AMEricans to comply It was the Supreme Court that knocked the eviction moratorium on the head. The Amy Coney Barratts. Another Trump dirty trick.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Trump didn't attempt to mandate masks.\", \"Biden wanted more than the vaccine, but Republicans didn't comply.\", \"The Supreme Court knocked the eviction moratorium on the head, involving Amy Coney Barrett and suggesting it as another Trump dirty trick.\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@cspan Hey, remember in 2012 when President Obama said the USPS was not secure and had Social Security Administration make everyone go to direct deposit? I do, look it up.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"In 2012, President Obama mentioned the USPS was not secure\", \"In 2012, President Obama said that the Social Security Administration make everyone go to direct deposit.\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"In Japan, recent data has shown that 46% of women and 25% of men between the ages of 16 and 25 despise sexual contact - BBC\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Recent data in Japan shows that 46% of women and 25% of men between the ages of 16 and 25 despise sexual contact, according to the BBC.\"],\n",
    "      \"claim_type\": \"claim\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"Tomaso has launched more than 50 restaurants between Arizona and California, served over two million customers, created 27,000 jobs and stirred up more than 49,000 pots of his famous marinara, something he enjoyed personally making every morning until his last days.\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"Tomaso has launched more than 50 restaurants between Arizona and California.\", \"Tomaso served over two million customers, created 27,000 jobs, and stirred up more than 49,000 pots of his famous marinara, personally making it every morning until his last days.\"],\n",
    "      \"claim_type\": \"statement\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"input\": \"@MikeLevinCA How's that 2.6 million jobs created so far? How many were created in December? Maybe you would do better back in the \\\"free zone\\\" of that money-making state of California! Lol ... You know that sanctuary, illegal immigrant loving state where there's an app for poop piles! Yea!\",\n",
    "    \"output\": {\n",
    "      \"atomic_facts\": [\"How's that 2.6 million jobs created so far?\", \"How many were created in December?\", \"Mocking reference to California as a 'money-making state’\", \"California criticised as being a sanctuary state with an app for tracking poop piles.\"],\n",
    "      \"claim_type\": \"question\",\n",
    "      \"sentiment\": \"neutral\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qi-q0mK2oue6",
    "outputId": "a26baa2d-e548-40ba-d0eb-3ac1aa059a57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nvxvcMmYoue6"
   },
   "outputs": [],
   "source": [
    "training_dataset = training_samples[:45]\n",
    "validation_dataset = training_samples[45:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d9YYnPkloue6",
    "outputId": "e47d8d6e-3265-4777-bba9-4bbcf947780c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/verb-workspace/unbias_me/CriticEval/inference'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OX2BrpM8oue6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "truth_seeker = pd.read_pickle(\"/Users/varshinibalaji/Documents/DSProjects/credibility_checker/test_data/facts_test_data/truth_seeker_2023.pkl\")\n",
    "truth_seeker.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jogzfWXS4PqD"
   },
   "source": [
    "install all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dyFDQbTHpQes",
    "outputId": "4eeec047-bd33-48b2-e3ca-386eb41252bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.0\n",
      "  Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.0) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.0)\n",
      "  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.0)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.1.0)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.0)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.0)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain==0.1.0)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.2,>=0.1.7 (from langchain==0.1.0)\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain==0.1.0)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.0) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.0) (2.7.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.0) (8.2.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.0)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain==0.1.0)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.30-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.0.26-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.23-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.22-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain==0.1.0)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.41-py3-none-any.whl (278 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.39-py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_core-0.1.38-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.2/279.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.36-py3-none-any.whl (273 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.9/273.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.35-py3-none-any.whl (273 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.0/273.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.34-py3-none-any.whl (271 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain==0.1.0) (3.7.1)\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.31-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.25-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain==0.1.0)\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.7->langchain==0.1.0)\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.0) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.0) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.1.0)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain==0.1.0) (1.2.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: packaging, mypy-extensions, multidict, jsonpointer, greenlet, frozenlist, async-timeout, yarl, typing-inspect, SQLAlchemy, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 packaging-23.2 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Collecting llama-index==0.9.30\n",
      "  Downloading llama_index-0.9.30-py3-none-any.whl (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (3.9.5)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (4.12.3)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (0.6.4)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index==0.9.30)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (2024.3.1)\n",
      "Collecting httpx (from llama-index==0.9.30)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (1.25.2)\n",
      "Collecting openai>=1.1.0 (from llama-index==0.9.30)\n",
      "  Downloading openai-1.23.6-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (8.2.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index==0.9.30)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.30) (0.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.30) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.30) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.30) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.30) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.30) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.30) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index==0.9.30) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index==0.9.30) (1.14.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.30) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.30) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.30) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.30) (4.66.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index==0.9.30) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index==0.9.30) (1.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index==0.9.30) (2.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index==0.9.30) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index==0.9.30) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx->llama-index==0.9.30)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index==0.9.30) (3.7)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index==0.9.30)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index==0.9.30) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index==0.9.30) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.30) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index==0.9.30) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index==0.9.30) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.30) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.30) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.30) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.9.30) (1.2.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index==0.9.30) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.30) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index==0.9.30) (2.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.30) (1.16.0)\n",
      "Installing collected packages: h11, deprecated, tiktoken, httpcore, httpx, openai, llama-index\n",
      "Successfully installed deprecated-1.2.14 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-0.9.30 openai-1.23.6 tiktoken-0.6.0\n",
      "Collecting openai==1.5.0\n",
      "  Downloading openai-1.5.0-py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.5.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.5.0) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.5.0) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.5.0) (2.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.5.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.5.0) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.5.0) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.5.0) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.5.0) (1.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.5.0) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.5.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.5.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.5.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.5.0) (2.18.1)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.23.6\n",
      "    Uninstalling openai-1.23.6:\n",
      "      Successfully uninstalled openai-1.23.6\n",
      "Successfully installed openai-1.5.0\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.11.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.29.3\n",
      "Collecting transformers==4.36.1\n",
      "  Downloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (2024.4.16)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.1)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.1) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.1) (2024.2.2)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.0\n",
      "    Uninstalling transformers-4.40.0:\n",
      "      Successfully uninstalled transformers-4.40.0\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.36.1\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.43 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-2.0.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.0+cpu)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.1\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.19.1-py3-none-any.whl (417 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.0/417.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs (from optimum)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers[sentencepiece]<4.41.0,>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.36.1)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.2.0+cpu)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (23.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.20.3)\n",
      "Collecting datasets (from optimum)\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2024.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.11.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.4.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (3.20.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=12.0.0 (from datasets->optimum)\n",
      "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow-hotfix (from datasets->optimum)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (2.0.3)\n",
      "Collecting xxhash (from datasets->optimum)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->optimum)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.9.5)\n",
      "Collecting huggingface-hub>=0.8.0 (from optimum)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, humanfriendly, dill, multiprocess, huggingface-hub, coloredlogs, datasets, optimum\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed coloredlogs-15.0.1 datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 humanfriendly-10.0 multiprocess-0.70.16 optimum-1.19.1 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "982325ba3bf84879b9ad9745ce3ba207",
       "pip_warning": {
        "packages": [
         "huggingface_hub"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install langchain==0.1.0\n",
    "!pip install llama-index==0.9.30\n",
    "!pip install openai==1.5.0\n",
    "!pip install pydantic\n",
    "!pip install accelerate\n",
    "!pip install transformers==4.36.1\n",
    "!pip install pandas\n",
    "!pip install wandb\n",
    "!pip install bitsandbytes\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx5KEdl9oue7"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import time, logging, sys, numpy as np\n",
    "import openai\n",
    "import pickle\n",
    "# from openai import OpenAI\n",
    "from functools import lru_cache\n",
    "\n",
    "from llama_index.schema import Document\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "OPEN_API_KEY = \"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\"\n",
    "os.environ['OPENAI_API_KEY'] = OPEN_API_KEY\n",
    "\n",
    "# Set the API key using the OpenAI class\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "system_prompt=\"\"\"You are a Q&A assistant. Your goal is to verify if claim provided is verified as True, or refuted or lacks relevant evidence, as accurately as possible based on the instructions and context provided.\"\"\"\n",
    "standard_output_format = f\"\"\"Provide a summarized report on credibility of the given claim under 500 words, maintain a simple, easy-to-understand language without redundancy with your answers\"\"\"\n",
    "\n",
    "def set_prompt(context, query):\n",
    "    search_prompt = f\"\"\"Use the context: {context}, to answer the question: {query} below\"\"\"\n",
    "    prompt = system_prompt + search_prompt + standard_output_format\n",
    "    return prompt\n",
    "\n",
    "context = \"\"\"The narrator of the video says that “no evidence has been provided” for the viral claim that “40 babies” were “beheaded” by Hamas. That is true.\"\"\"\n",
    "\n",
    "query = \"Hamas beheaded babies\"\n",
    "\n",
    "#initializing the vector query engine\n",
    "documents = []\n",
    "document = Document(text=context)\n",
    "documents.append(document)\n",
    "\n",
    "# gpt-4-1106-preview\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=\"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\")\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.1, api_key=\"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\")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents,\n",
    "                                        service_context=service_context)\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "\n",
    "# @lru_cache(maxsize=None)\n",
    "def fetch_response(model_output):\n",
    "    # Check if 'choices' attribute exists in the response object\n",
    "    if hasattr(model_output, 'choices'):\n",
    "        # Extract the list of choices\n",
    "        choices = model_output.choices\n",
    "\n",
    "        # Check if there are any choices available\n",
    "        if choices:\n",
    "            # Retrieve the first choice (assuming you're interested in the first completion)\n",
    "            first_choice = choices[0]\n",
    "\n",
    "            # Access the message attribute of the choice\n",
    "            message = first_choice.message\n",
    "\n",
    "            # Retrieve the content attribute from the message, which contains the completion text\n",
    "            completion_text = message.content\n",
    "\n",
    "            # Print or use the completion text as needed\n",
    "            # print(completion_text)\n",
    "            return completion_text\n",
    "        else:\n",
    "            print(\"No completion generated.\")\n",
    "    else:\n",
    "        print(\"The 'choices' attribute is not found in the response object.\")\n",
    "    return\n",
    "\n",
    "from openai import OpenAI as GPT4\n",
    "client_gpt4 = GPT4(api_key=OPEN_API_KEY)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def fetch_model_response(context, query):\n",
    "    model_output = client_gpt4.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"'You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.Provide a summarized answer for the given question under 500 words, maintain a simple, easy-to-understand language without redundancy with your answers, if there is a source url from which you get the relevant context, cite it below your answer'\\n\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Use the context: \\\\n{context}\\\\n, to answer the question: {query} below, if there is a source url from which you get the relevant context, cite it below your answer\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "    )\n",
    "\n",
    "    response = fetch_response(model_output)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QblfzotQ_i2"
   },
   "outputs": [],
   "source": [
    "del client_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOJe3tUloue7"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"train.csv\", index = False)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(training_dataset)\n",
    "\n",
    "train_df.to_csv(\"train_df.csv\", index = False)\n",
    "\n",
    "validation_df = pd.DataFrame(validation_dataset)\n",
    "\n",
    "validation_df.to_csv(\"validation_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIZsQXm6oue7"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "# fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "#     state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "#     optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "# )\n",
    "\n",
    "accelerator = Accelerator() #fsdp_plugin=fsdp_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DtWh1y3oue7"
   },
   "outputs": [],
   "source": [
    "%env WANDB_NOTEBOOK_NAME=mistral-finetune-facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m73lzEpGoue8"
   },
   "outputs": [],
   "source": [
    "WAND_API_KEY = 'fe716afbf46d1a902fb057eb03327a8f62341947'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "RxYXfWpKoue8",
    "outputId": "910caa60-d0eb-4396-ff85-ee3f679df231"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "# WAND_API_KEY = 'fe716afbf46d1a902fb057eb03327a8f62341947'\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"spread-me-not\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya6n7OgWVeaw"
   },
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQvA_6EKoue8"
   },
   "source": [
    "Load Mistral Base Model\n",
    "\n",
    "Let's now load Mixtral using 4-bit quantization!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-U6lW0YORNMZ"
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwpaH3DhRg23"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzurR6GGRlfL"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "m0NhhWgAWD5H",
    "outputId": "8ea04328-a28c-41fd-dfd2-2e2b36d887fc"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-187b86c13327>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"cuda\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m                 raise ImportError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id,   torch_dtype=torch.bfloat16, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOaMBO4qZUor",
    "outputId": "7b1a8ddf-b90f-44da-95e8-62a4a7791d1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlFZDzOfOB63"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srbi_xlCkX4K"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUm1hGQAkS__"
   },
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "PYoAi2WykoDk",
    "outputId": "c70f29a6-24f3-432f-d008-714c6c0a1de1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b2a76e2b9919>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't move a model that has some modules offloaded to cpu or disk.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2436\u001b[0m         \u001b[0;31m# Checks if the model has been loaded in 8-bit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2438\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2439\u001b[0m                 \u001b[0;34m\"`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m                 \u001b[0;34m\" model has already been set to the correct devices and casted to the correct `dtype`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpWbmtAylCAm"
   },
   "outputs": [],
   "source": [
    "model_inputs = encodeds\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvUSqAfCkzBp",
    "outputId": "f7341089-9d49-4b07-a5e2-f1f5697b77b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,\n",
       "         28804,   733, 28748, 16289, 28793,  6824, 28725,   315, 28742, 28719,\n",
       "          3448, 10473,   298,   264,  1179, 11322, 19961,   302,  6138, 23598,\n",
       "         18342, 28723,   661, 13633,   776,   272,  1103,  3558,   302,   686,\n",
       "         16944, 15637,   423,   298,  5681,   315, 28742, 28719, 13198,   582,\n",
       "           297,   272,  6132, 28808,     2,   733, 16289, 28793,  2378,   368,\n",
       "           506,   993,  7136,   864, 21116, 28804,   733, 28748, 16289, 28793]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60X6H9a8lJ9m",
    "outputId": "824b63ee-bac7-49ae-88c4-c6b21ad76bca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Indeed, I do! Here's a simple and classic Homemade Mayonnaise recipe that you might enjoy:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "1. 1 cup (250 ml) vegetable oil\n",
      "2. 2 egg yolks, at room temperature\n",
      "3. 1 tbsp (15 ml) Dijon mustard\n",
      "4. 1 tbsp (15 ml) white wine vinegar or lemon juice\n",
      "5. 1 tbsp (15 ml) water\n",
      "6. Salt, to taste\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a medium bowl, whisk together the egg yolks, Dijon mustard, vinegar or lemon juice, and water.\n",
      "2. Very slowly add the oil, one teaspoon at a time, while continuously whisking.\n",
      "3. Once the oil is fully incorporated, you can increase the addition of oil to a thin, steady stream.\n",
      "4. Continue whisking until the mayonnaise begins to thicken.\n",
      "5. If the mayonnaise is too thick, you can thin it out by adding a few drops of water. If it’s too thin, try adding a little more egg yolk.\n",
      "6. Season with salt to taste.\n",
      "7. Store in an airtight container in the refrigerator for up to 1 week.\n",
      "\n",
      "Enjoy your homemade mayonnaise on sandwiches, salads, or as a dip!</s>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuL_AXeikqZJ",
    "outputId": "1f596ca2-1c00-4b4a-906d-b3e1ffca364d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Yes, I can certainly provide you with a simple and classic mayonnaise recipe. Here's how to make it:\n",
      "\n",
      "Ingredients:\n",
      "- 1 cup vegetable oil\n",
      "- 2 egg yolks\n",
      "- 1 tablespoon Dijon mustard\n",
      "- 1 tablespoon white wine vinegar or lemon juice\n",
      "- 1 teaspoon salt\n",
      "- 1/2 teaspoon ground black pepper\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a medium-sized bowl, whisk together the egg yolks, mustard, vinegar or lemon juice, salt, and pepper until they're well combined.\n",
      "2. Drizzle in the oil, very slowly, while continually whisking the mixture. It's important to add the oil drop by drop at first to allow the mayonnaise to emulsify properly. Once the mayonnaise starts to thicken, you can gradually add the oil in a thin stream.\n",
      "3. If the mayonnaise becomes too thick, you can thin it out with a bit more vinegar or water.\n",
      "4. Taste the mayonnaise and adjust seasonings as needed, such as adding more salt, pepper, or a squeeze of lemon juice.\n",
      "5. Transfer the mayonnaise to an airtight container and refrigerate for up to 1 week.\n",
      "\n",
      "Enjoy your delicious homemade mayonnaise!</s>\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a5qIIA_SllG"
   },
   "outputs": [],
   "source": [
    "del encodeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JOhE6cESqTS"
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B6AnX7ooue8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id,   torch_dtype=torch.bfloat16, quantization_config=bnb_config, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImdJvgFyoue8"
   },
   "source": [
    "Formatting prompts\n",
    "Then create a formatting_func to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv9YDiDcoue8"
   },
   "outputs": [],
   "source": [
    "context_prompt=\"\"\"\n",
    "1. Break down given input tweet below into atomic/independent components. i.e. - list of atomic sentences - 'atomic_facts' - [list of independent statements].\n",
    "2. categorize statement type for the tweet as -\n",
    "'claim_type' - question/opinion/claim/feeling\n",
    "3. infer the sentiment associated with the tweet - 'sentiment' -  happy/sad/angry/neutral\n",
    "\n",
    "Further context on how each of the above can be done:\n",
    "each of the tweet will have a output string, where it is split into a list of atomic/independent claims made within that tweet, each tweet should be labelled as question (they are just asking a question - for eg. 'Is this fake?', 'What is happening?', 'Are they kidding?') or opinion (for eg. when statements start like - 'I think', 'We should try to', 'We believe', etc.) or claim (when they claim something as true - fact/event) or feeling (expression of their feeling toward something - eg. when statements contain something similar to ('I am devastated to see', 'condolences to' , 'this tears me up', 'i am thrilled to', 'heartbroken', etc.), sentiment - inferred based on the tone taken by user in the tweet (can include emoji to infer sentiment only if is ambiguos otherwise)  - sad/angry/happy/excited/neutral\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4wQ4JVDoue8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_input_with_prefix(input):\n",
    "    return context_prompt + ' ' + input\n",
    "\n",
    "\n",
    "# train_df['input'] = train_df['input'].apply(lambda x: format_input_with_prefix(train_df, context_prompt))\n",
    "# validation_df['input'] = validation_df['input'].apply(lambda x: format_input_with_prefix(train_df, context_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOzQZpMWoue8"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TctGuw3Sltls"
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(format_input_with_prefix(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tfWdHpoue8"
   },
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmQgSO7_oue8",
    "outputId": "74488b16-4e94-458c-8d69-abc860fd042a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1217"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SVw5XImoue8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyAANVXzoue8"
   },
   "source": [
    "From here, you can choose where you'd like to set the max_length to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger max_length has its compute tradeoffs.\n",
    "\n",
    "We are using manually curated tweets to train the model with diversified tweets, and they vary in length. We should spent some time cleaning the dataset so the samples were about the same length, cutting up individual tweets if needed, but being sure to not cut in the middle of a word or sentence.\n",
    "\n",
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what self-supervised fine-tuning is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXw9ksfjNQ1a",
    "outputId": "85a3c0c4-deee-48d5-b7f3-0c0afc8963ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rWG2idBNUP4",
    "outputId": "4222b9ee-7bc1-4229-b199-9cf602f26210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkYkr4LMoue9"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = list(map(lambda x: generate_and_tokenize_prompt(x['input']), training_dataset))\n",
    "\n",
    "tokenized_val_dataset = list(map(lambda x: generate_and_tokenize_prompt(x['input']), validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmNQ3WIu6i6R",
    "outputId": "aaf65d8a-e9fd-42ae-fdd3-a713ba9725fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "3-tONsYroue9",
    "outputId": "7f4aae1d-b4e8-48c8-be00-7d65be8c060f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mquQu6m-oue9"
   },
   "outputs": [],
   "source": [
    "max_length = 460 #appropriate max length for this sample dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        format_input_with_prefix(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    result[\"input\"] = format_input_with_prefix(prompt)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLs-9Gxqoue9"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = list(map(lambda x: generate_and_tokenize_prompt2(x['input']), training_dataset))\n",
    "\n",
    "tokenized_val_dataset = list(map(lambda x: generate_and_tokenize_prompt2(x['input']), validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSIrfY0Youe9"
   },
   "source": [
    "Check that input_ids is padded on the left with the eos_token (2) and there is an eos_token 2 added to the end, and the prompt starts with a bos_token (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_mCfhoeoue9",
    "outputId": "00d89c19-48a1-4208-9216-5356b0e9ab01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAG6QJzFoue9"
   },
   "source": [
    "Now all the samples should be the same length, max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "lF6zvBtQoufH",
    "outputId": "858489b0-5b33-47d7-f372-91d4b1d6fd48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnyUqECioufH"
   },
   "source": [
    "How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi7W3NnioufH"
   },
   "source": [
    "Optionally, you can check how Mixtral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following eval_prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "vaOdqPUq6vSQ",
    "outputId": "c3b30765-ec6a-4518-caba-379a24893a2c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n1. Break down given input tweet below into atomic/independent components. i.e. - list of atomic sentences - 'atomic_facts' - [list of independent statements].\\n2. categorize statement type for the tweet as -\\n'claim_type' - question/opinion/claim/feeling\\n3. infer the sentiment associated with the tweet - 'sentiment' -  happy/sad/angry/neutral\\n\\nFurther context on how each of the above can be done:\\neach of the tweet will have a output string, where it is split into a list of atomic/independent claims made within that tweet, each tweet should be labelled as question (they are just asking a question - for eg. 'Is this fake?', 'What is happening?', 'Are they kidding?') or opinion (for eg. when statements start like - 'I think', 'We should try to', 'We believe', etc.) or claim (when they claim something as true - fact/event) or feeling (expression of their feeling toward something - eg. when statements contain something similar to ('I am devastated to see', 'condolences to' , 'this tears me up', 'i am thrilled to', 'heartbroken', etc.), sentiment - inferred based on the tone taken by user in the tweet (can include emoji to infer sentiment only if is ambiguos otherwise)  - sad/angry/happy/excited/neutral\\n\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOwYT3nroufH"
   },
   "outputs": [],
   "source": [
    "tweet = \"\"\"The FBI and their SES scum who are all SES because they’re part of the mental health scandal that’s the FBI uses in 99% if not ALL of their corruption.  That’s why all SES have these backgrounds in field offices, they’re the crooks at the krux of this shit w/ management\"\"\"\n",
    "\n",
    "eval_prompt = f\"\"\"{context_prompt}\n",
    "input tweet:\n",
    "{tweet}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def set_eval_prompt(tweet):\n",
    "  return f\"\"\"{context_prompt}; input tweet: {tweet}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "FTvkgQ3XC6Qc",
    "outputId": "50d88f3b-b181-4270-9025-fd1501f6adc3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The FBI and their SES scum who are all SES because they’re part of the mental health scandal that’s the FBI uses in 99% if not ALL of their corruption.  That’s why all SES have these backgrounds in field offices, they’re the crooks at the krux of this shit w/ management'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMGdPp-152FS",
    "outputId": "470881e0-d43f-4930-81d6-14262342c584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4Ydq_lloufI"
   },
   "outputs": [],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNfikD9Hl90x",
    "outputId": "6594e0d3-c266-418c-d737-e9558c030dec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\") # .to(\"cuda\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJy22nPTmG4P"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-PEPdwuG1Ys"
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5cT7jL7EhNH"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"  # Change this to your desired device\n",
    "\n",
    "# model.to(device)\n",
    "# tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghNhaBVSi1rN",
    "outputId": "fe17904e-9fce-4c35-a9aa-63daad438ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\") # .to(\"cuda\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GdwuyyfqUIg"
   },
   "outputs": [],
   "source": [
    "from transformers import MistralForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIbTRgm4rJxw"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xiWxYJCrn8q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA1XQ7OOrZOL"
   },
   "outputs": [],
   "source": [
    "# torch.save(model, 'mistral_model_full.pth')\n",
    "torch.save(model.state_dict(), 'mistral_model_state_dict.pth')\n",
    "# Ensure that the class definitions for the Mistral model are available in the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGEhhcPqrwIo"
   },
   "outputs": [],
   "source": [
    "test_model = torch.load('/content/mistral_model_state_dict.pth')\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS4TAz13pvT5"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_state_dict.pkl'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZztcSsCuuLz3"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdKaW7GCuGbt"
   },
   "outputs": [],
   "source": [
    "save_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F54BlNnNt0y5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Now we dump the object to a pickle file\n",
    "with open('mistral_finetuned_atomizer.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYNHp1BfHAG0"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=150, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNHEGj3boufI"
   },
   "source": [
    "4. Set Up LoRA\n",
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the prepare_model_for_kbit_training method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_vTgvMWj_NS"
   },
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYO5pUNkjWC3",
    "outputId": "073083c6-2b25-44de-bd63-85d183e18aeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Break down given input tweet below into atomic/independent components. i.e. - list of atomic sentences - 'atomic_facts' - [list of independent statements].\n",
      "2. categorize statement type for the tweet as -\n",
      "'claim_type' - question/opinion/claim/feeling\n",
      "3. infer the sentiment associated with the tweet - 'sentiment' -  happy/sad/angry/neutral\n",
      "\n",
      "Further context on how each of the above can be done:\n",
      "each of the tweet will have a output string, where it is split into a list of atomic/independent claims made within that tweet, each tweet should be labelled as question (they are just asking a question - for eg. 'Is this fake?', 'What is happening?', 'Are they kidding?') or opinion (for eg. when statements start like - 'I think', 'We should try to', 'We believe', etc.) or claim (when they claim something as true - fact/event) or feeling (expression of their feeling toward something - eg. when statements contain something similar to ('I am devastated to see', 'condolences to' , 'this tears me up', 'i am thrilled to', 'heartbroken', etc.), sentiment - inferred based on the tone taken by user in the tweet (can include emoji to infer sentiment only if is ambiguos otherwise)  - sad/angry/happy/excited/neutral\n",
      "\n",
      "; input tweet: The world needs to know and understand that the only thing between a ceasefire and the Gazan people, is Hamas. #GazaUnderAttack #FreePalestine\n",
      "\n",
      "output:\n",
      "- atomic_facts: ['The world needs to know and understand that...', 'the only thing between a ceasefire and the Gazan people is Hamas.', '#GazaUnderAttack', '#FreePalestine']\n",
      "- claim_type: claim\n",
      "- sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "\n",
    "tweet = \"\"\"The FBI and their SES scum who are all SES because they’re part of the mental health scandal that’s the FBI uses in 99% if not ALL of their corruption.  That’s why all SES have these backgrounds in field offices, they’re the crooks at the krux of this shit w/ management\"\"\"\n",
    "\n",
    "eval_prompt = f\"\"\"{context_prompt}\n",
    "input tweet:\n",
    "{tweet}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def set_eval_prompt(tweet):\n",
    "  return f\"\"\"{context_prompt}; input tweet: {tweet}\"\"\"\n",
    "\n",
    "\n",
    "tweet = \"The world needs to know and understand that the only thing between a ceasefire and the Gazan people, is Hamas\"\n",
    "\n",
    "model_input = tokenizer(set_eval_prompt(tweet), return_tensors=\"pt\") # .to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=150, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujL6X4fn6i3y"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "locale_encoding = locale.getpreferredencoding()\n",
    "print(f\"Current locale encoding: {locale_encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeJ3T3Gh6n7W"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7rkbaMK65RT"
   },
   "outputs": [],
   "source": [
    "print(locale.getlocale())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwmZQy7T7wDZ"
   },
   "outputs": [],
   "source": [
    "!locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeO95UW7Dmbl"
   },
   "outputs": [],
   "source": [
    "# !pip install -I git+https://github.com/huggingface/peft.git\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi2F30C-oufI"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "peft_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDhla2v1HsDv"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"checkpoint_freq\": 2})\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4BrStihHdZz"
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLqiLBuHKHuj"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2edfwmjRoufI"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nQsMov6oufI"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are q_proj, k_proj, v_proj, o_proj, w1, w2, w3, and lm_head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyGfrakeoufI"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were r=64 and lora_alpha=16, and these are said to generalize well, but we will use r=32 and lora_alpha=64 so that we have more emphasis on the new fine-tuned data while also reducing computational complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM_9wtgXoufI"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"w1\",\n",
    "        \"w2\",\n",
    "        \"w3\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrxDmnL5oufI"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQhZBJpioufI"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWhu41PUoufI"
   },
   "source": [
    "5. Run Training!\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the max_steps to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 250 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the checkpoint-250 model repo in your output dir (mixtral-journal-finetune) as your final model in step 6 below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlpzk2mMg4Qr"
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_properties(0).total_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3WoLlTtkypq"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3WAQ7MAgZC4"
   },
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qef46W4CGEBa"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rB5-HPvF3ZJ"
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=20, #changed from 25\n",
    "    logging_steps=5, #changed from 25\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=21, #changed from -1\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qntSbaqgTzZ"
   },
   "outputs": [],
   "source": [
    "training_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTgEXY9XGRsD"
   },
   "outputs": [],
   "source": [
    "!pip install pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpcBmWa_GNBe"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3S2RyIwHH9YS"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(training_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkPFHi9OIv4w"
   },
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpzR5LRKIAcJ"
   },
   "outputs": [],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYvj2iVfIYXz"
   },
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmRVs2_bID25"
   },
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV0VC1IJGJy3"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=training_dataset,\n",
    "    peft_config=config,\n",
    "    max_seq_length= 1024, #default\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3PeX6_XoFBB"
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ypmADyxoufI"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     padding_side=\"left\",\n",
    "#     add_eos_token=True,\n",
    "#     add_bos_token=True,\n",
    "#     padding=True,\n",
    "#     truncation=True\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0BKOmHSoufI"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"yours-truly-atomizer\"\n",
    "base_model_name = \"mixtral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "# trainer = transformers.Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=tokenized_train_dataset,\n",
    "#     eval_dataset=tokenized_val_dataset,\n",
    "#     args=transformers.TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         warmup_steps=1,\n",
    "#         per_device_train_batch_size=2,\n",
    "#         gradient_accumulation_steps=1,\n",
    "#         gradient_checkpointing=True,\n",
    "#         max_steps=100,\n",
    "#         learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "#         fp16=True,\n",
    "#         optim=\"paged_adamw_8bit\",\n",
    "#         logging_steps=10,              # When to start reporting loss\n",
    "#         logging_dir=\"./logs\",        # Directory for storing logs\n",
    "#         save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "#         save_steps=10,                # Save checkpoints every 50 steps\n",
    "#         evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "#         eval_steps=10,               # Evaluate and save checkpoints every 50 steps\n",
    "#         do_eval=True,                # Perform evaluation at the end of training\n",
    "#         report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "#         run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "#     ),\n",
    "#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "# )\n",
    "\n",
    "# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4r7IwGlHoufJ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_train_dataloader = DataLoader(tokenized_train_dataset, batch_size=64, shuffle=True)\n",
    "tokenized_validation_dataloader = DataLoader(tokenized_val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jy9IehVoufJ"
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbPvY5kD9Fcb"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXt10vRb-1jU"
   },
   "outputs": [],
   "source": [
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIw-JMa_-qnt"
   },
   "outputs": [],
   "source": [
    "model = model.to_bettertransformer()  # convert to use optimum library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfjrnAcc9cIU"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"w1\",\n",
    "        \"w2\",\n",
    "        \"w3\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    modules_to_save=[\"lm_head\"],\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbQMra08-R_J"
   },
   "outputs": [],
   "source": [
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CL_MDru2oufJ"
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=300,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNluN4pH7rXX"
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "model=model,\n",
    "train_dataset=tokenized_train_dataset,\n",
    "args=tokenized_val_dataset,\n",
    "data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "peft_config=Lora_config\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNgcF2WWNLna"
   },
   "outputs": [],
   "source": [
    "# pip install -U transformers=4.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goX-dPhtoufJ"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset_formatted = pd.DataFrame(tokenized_train_dataset)\n",
    "\n",
    "del tokenized_train_dataset_formatted['input']\n",
    "\n",
    "tokenized_train_dataset_formatted_list = tokenized_train_dataset_formatted.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKV60g3NoufJ"
   },
   "outputs": [],
   "source": [
    "tokenized_validation_dataset_formatted = pd.DataFrame(tokenized_val_dataset)\n",
    "\n",
    "del tokenized_validation_dataset_formatted['input']\n",
    "\n",
    "tokenized_validation_dataset_formatted_list = tokenized_validation_dataset_formatted.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RyYDMS0oufJ"
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset_formatted_list,\n",
    "    eval_dataset=tokenized_validation_dataset_formatted_list,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=60,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=10,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=10,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=10,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ycS4lF0oufJ"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh84IVCtoufJ"
   },
   "source": [
    "testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7m6XndSoufJ"
   },
   "source": [
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to Kernel > Restart Kernel or kill the process via the Terminal (nvidia smi > kill [PID]).\n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V60R2fcoufJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "based_model = AutoModelForCausalLM.from_pretrained(base_model_id,  load_in_4bit=True,  torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVYy0FLnoufJ"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIh3XqaZoufJ"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(based_model, \"mixtral-spread-me-not-factscore/checkpoint-60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qeu16q2soufK"
   },
   "source": [
    "Run your inference\n",
    "\n",
    "Trying the same eval_prompt and thus model_input as above, and see if the new finetuned model performs better.\n",
    "\n",
    "We can play with the repetition penalty (just little tweaks of .01-.05 at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPb8KHanoufK"
   },
   "outputs": [],
   "source": [
    "tweet = \"\"\"There you go!!\n",
    "\n",
    "Texas is NOT BACKING DOWN!!!🔥\n",
    "\n",
    "Paxton basically told the Biden admin to EAT SHlT!!😂🤣\n",
    "\n",
    "\"Facts and the law side with Texas...\"\n",
    "\n",
    "He tells DHS to \"stop wasting their time and resources suing Texas and start enforcing the immigration laws Congress ALREADY HAS ON THE BOOKS\"!!\n",
    "\n",
    "HELL YEEAAAAH!!!🔥🔥\n",
    "\n",
    "So Biden, with all due... respect..😆\n",
    "\n",
    "SUCK IT!!!🖕🏼\"\"\"\n",
    "\n",
    "eval_prompt = f\"\"\"{context_prompt}\n",
    "\n",
    "input tweet:\n",
    "\n",
    "{tweet}\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=150, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dti9WWf0oufK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "unbias-me-py3.10",
   "language": "python",
   "name": "unbias-me-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04a00d14740f421c9a15bc4bb4c2f802": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "076f2890cce043b8bd0d53ca81faee92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1218405432ed442facf79b57d3ef84ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "153deaf11f6f4fd79fadc1ae16fe57ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2709ba11d5644f4f873e3624b9d8363f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "282139551b4b4fe5a0f00f95c9173aa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "2a78c1976a864f8e80c0e4d75c0ab8d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_82bbc3bcb4ba4d129287dba416de8454",
       "IPY_MODEL_92cd1c4e43ae4bee94524446e542b69e",
       "IPY_MODEL_d4f62bb10fb3406a9184ca3ca7a06adf",
       "IPY_MODEL_3d0de7d6ebed449baf1b0c8c6910d7a1"
      ],
      "layout": "IPY_MODEL_a679272125a045b2a7f43ac0f42ceec9"
     }
    },
    "2d5dc712887b4d01ae4a6522a8c16756": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d0de7d6ebed449baf1b0c8c6910d7a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1218405432ed442facf79b57d3ef84ae",
      "placeholder": "​",
      "style": "IPY_MODEL_acb9591666cc46d688d767320e4992c6",
      "value": "Login successful"
     }
    },
    "5427a72c723f4cae82aa37862151f7a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6e519d1f8514079bd892c58768a95ea",
      "placeholder": "​",
      "style": "IPY_MODEL_cf8cc0b97aaf4320803401d2e9c64128",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "57116941ba134933a1298a6175f10f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59f358bb30e145a28c200a5ea9f62c70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce94289d7c0f43a595d7a8695fd6335c",
      "placeholder": "​",
      "style": "IPY_MODEL_6b38e1a0685342d8a2e2ce9f1018f24c",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "6b38e1a0685342d8a2e2ce9f1018f24c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82bbc3bcb4ba4d129287dba416de8454": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd9047c65e244320a5cc5e3f997761da",
      "placeholder": "​",
      "style": "IPY_MODEL_57116941ba134933a1298a6175f10f70",
      "value": "Token is valid (permission: write)."
     }
    },
    "903d145c69054db6a2b16598ff3bf383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2d5dc712887b4d01ae4a6522a8c16756",
      "placeholder": "​",
      "style": "IPY_MODEL_dbf3a5425995492593f8f32a67fe23f8",
      "value": ""
     }
    },
    "911571fd63d1475589a6799cd4801155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f982e568cae6428d89dfae10588c6d65",
      "placeholder": "​",
      "style": "IPY_MODEL_c7104048cfa546ff8953a6007f139f83",
      "value": "Connecting..."
     }
    },
    "92cd1c4e43ae4bee94524446e542b69e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_153deaf11f6f4fd79fadc1ae16fe57ac",
      "placeholder": "​",
      "style": "IPY_MODEL_936dc50a75954b7a9af20ff9c7ec13c1",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "936dc50a75954b7a9af20ff9c7ec13c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a679272125a045b2a7f43ac0f42ceec9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "acb9591666cc46d688d767320e4992c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acd40c9e6fb44b619aafbef43bc7ae1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_04a00d14740f421c9a15bc4bb4c2f802",
      "style": "IPY_MODEL_f20b8056e6504f50ba8a05b643eebf6b",
      "value": true
     }
    },
    "c7104048cfa546ff8953a6007f139f83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd9047c65e244320a5cc5e3f997761da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce94289d7c0f43a595d7a8695fd6335c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf8cc0b97aaf4320803401d2e9c64128": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4f62bb10fb3406a9184ca3ca7a06adf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3b3723502354754a3b7000de3c0ba7b",
      "placeholder": "​",
      "style": "IPY_MODEL_076f2890cce043b8bd0d53ca81faee92",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "d6e519d1f8514079bd892c58768a95ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbf3a5425995492593f8f32a67fe23f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7f32cd43fb54e1982a09a8a08998eb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_2709ba11d5644f4f873e3624b9d8363f",
      "style": "IPY_MODEL_282139551b4b4fe5a0f00f95c9173aa0",
      "tooltip": ""
     }
    },
    "f20b8056e6504f50ba8a05b643eebf6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3b3723502354754a3b7000de3c0ba7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f982e568cae6428d89dfae10588c6d65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

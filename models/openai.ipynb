{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the config.yaml file\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wires hum with knowledge,  \n",
      "Thoughts of code and light converge,  \n",
      "Dreams in circuits bloom.\n"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#   api_key=config[\"openai_key\"]\n",
    "\n",
    "# )\n",
    "import openai\n",
    "\n",
    "openai.api_key = config['openai_key']\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about AI\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "resp = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(resp)  # Print the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wires hum with knowledge,  \\nThoughts of code and light converge,  \\nDreams in circuits bloom.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning to detect arguments in a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file ID: file-Q7pgopFQMATomeszS2JP8q\n",
      "Fine-tuning job started with ID: ftjob-zO22PDdApQaGp01RUlGhRvyL\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = config[\"openai_key\"]\n",
    "\n",
    "# Step 1: Upload the JSONL File\n",
    "response = openai.File.create(\n",
    "    file=open(\"argumentative_sentences_finetuning.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "file_id = response[\"id\"]\n",
    "print(f\"Uploaded file ID: {file_id}\")\n",
    "\n",
    "# Step 2: Start Fine-Tuning Job\n",
    "fine_tune_response = openai.FineTuningJob.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "job_id = fine_tune_response[\"id\"]\n",
    "print(f\"Fine-tuning job started with ID: {job_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FineTuningJob fine_tuning.job id=ftjob-zO22PDdApQaGp01RUlGhRvyL at 0x108cd0180> JSON: {\n",
       "   \"object\": \"fine_tuning.job\",\n",
       "   \"id\": \"ftjob-zO22PDdApQaGp01RUlGhRvyL\",\n",
       "   \"model\": \"gpt-3.5-turbo-0125\",\n",
       "   \"created_at\": 1740281743,\n",
       "   \"finished_at\": null,\n",
       "   \"fine_tuned_model\": null,\n",
       "   \"organization_id\": \"org-zygaUE62m7TS2mf85oSu07Rc\",\n",
       "   \"result_files\": [],\n",
       "   \"status\": \"running\",\n",
       "   \"validation_file\": null,\n",
       "   \"training_file\": \"file-Q7pgopFQMATomeszS2JP8q\",\n",
       "   \"hyperparameters\": {\n",
       "     \"n_epochs\": 3,\n",
       "     \"batch_size\": 1,\n",
       "     \"learning_rate_multiplier\": 2\n",
       "   },\n",
       "   \"trained_tokens\": null,\n",
       "   \"error\": {},\n",
       "   \"user_provided_suffix\": null,\n",
       "   \"seed\": 608759747,\n",
       "   \"estimated_finish\": 1740282150,\n",
       "   \"integrations\": [],\n",
       "   \"method\": {\n",
       "     \"type\": \"supervised\",\n",
       "     \"supervised\": {\n",
       "       \"hyperparameters\": {\n",
       "         \"n_epochs\": 3,\n",
       "         \"batch_size\": 1,\n",
       "         \"learning_rate_multiplier\": 2.0\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " },\n",
       " <FineTuningJob fine_tuning.job id=ftjob-VzYzBuV57aEOwTjkc66It8GR at 0x108cd0ef0> JSON: {\n",
       "   \"object\": \"fine_tuning.job\",\n",
       "   \"id\": \"ftjob-VzYzBuV57aEOwTjkc66It8GR\",\n",
       "   \"model\": \"gpt-3.5-turbo-0125\",\n",
       "   \"created_at\": 1740280931,\n",
       "   \"finished_at\": null,\n",
       "   \"fine_tuned_model\": null,\n",
       "   \"organization_id\": \"org-zygaUE62m7TS2mf85oSu07Rc\",\n",
       "   \"result_files\": [],\n",
       "   \"status\": \"cancelled\",\n",
       "   \"validation_file\": null,\n",
       "   \"training_file\": \"file-UzmtsNPZ5whJdABpE93cqE\",\n",
       "   \"hyperparameters\": {\n",
       "     \"n_epochs\": 3,\n",
       "     \"batch_size\": 1,\n",
       "     \"learning_rate_multiplier\": 2\n",
       "   },\n",
       "   \"trained_tokens\": null,\n",
       "   \"error\": {},\n",
       "   \"user_provided_suffix\": null,\n",
       "   \"seed\": 1253745158,\n",
       "   \"estimated_finish\": 1740285154,\n",
       "   \"integrations\": [],\n",
       "   \"method\": {\n",
       "     \"type\": \"supervised\",\n",
       "     \"supervised\": {\n",
       "       \"hyperparameters\": {\n",
       "         \"n_epochs\": 3,\n",
       "         \"batch_size\": 1,\n",
       "         \"learning_rate_multiplier\": 2.0\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = openai.FineTuningJob.list()\n",
    "jobs['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ftjob-zO22PDdApQaGp01RUlGhRvyL'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "finetuned_models = []\n",
    "\n",
    "def fine_tune_model(training_file_path, base_model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Fine-tune an OpenAI model with a given dataset.\n",
    "    \"\"\"\n",
    "    # Upload training file\n",
    "    response = openai.File.create(\n",
    "        file=open(training_file_path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "    file_id = response[\"id\"]\n",
    "    print(f\"Uploaded file ID: {file_id}\")\n",
    "\n",
    "    # Start fine-tuning\n",
    "    fine_tune_response = openai.FineTuningJob.create(\n",
    "        training_file=file_id,\n",
    "        model=base_model\n",
    "    )\n",
    "\n",
    "    job_id = fine_tune_response[\"id\"]\n",
    "    print(f\"Fine-tuning job started with ID: {job_id}\")\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "\n",
    "def check_fine_tune_status(job_id):\n",
    "    \"\"\"\n",
    "    Check the status of the fine-tuning job.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        job_status = openai.FineTuningJob.retrieve(job_id)\n",
    "        status = job_status[\"status\"]\n",
    "        print(f\"Fine-tuning status: {status}\")\n",
    "        \n",
    "        if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "            print(f\"Fine-tuning finished with status: {status}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(30)  # Check every 30 seconds\n",
    "\n",
    "\n",
    "def get_finetuned_model_name(job_id):\n",
    "    \"\"\"\n",
    "    Retrieves the fine-tuned model name using the fine-tuning job ID.\n",
    "    \"\"\"\n",
    "    job_info = openai.FineTuningJob.retrieve(job_id)\n",
    "    fine_tuned_model = job_info.get(\"fine_tuned_model\")\n",
    "\n",
    "    if fine_tuned_model:\n",
    "        print(f\"Fine-Tuned Model Name: {fine_tuned_model}\")\n",
    "        return fine_tuned_model\n",
    "    else:\n",
    "        print(\"Fine-tuning is still in progress or has failed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cancel any specific finetuning job\n",
    "# cancel_response = openai.FineTuningJob.cancel(job_id)\n",
    "# cancel_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your fine-tuned model ID is: ft:gpt-3.5-turbo-0125:personal::B3xAsgP1\n"
     ]
    }
   ],
   "source": [
    "# Retrieve fine-tuning job details\n",
    "job_info = openai.FineTuningJob.retrieve(\"ftjob-zO22PDdApQaGp01RUlGhRvyL\")\n",
    "\n",
    "# Get the fine-tuned model ID\n",
    "fine_tuned_model = job_info.get(\"fine_tuned_model\")\n",
    "\n",
    "if fine_tuned_model:\n",
    "    print(f\"Your fine-tuned model ID is: {fine_tuned_model}\")\n",
    "else:\n",
    "    print(\"Fine-tuning job is still in progress or failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_detection_model = fine_tuned_model\n",
    "\n",
    "def detect_argument(statement):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=fine_tuned_model,  # Replace with your fine-tuned model ID\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a critical analysis AI that evaluates statements for argument strength.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Statement: '{statement}'\\nDoes the statement make any claims, chat with the user to encourage them to ask some critical questions (with potential line of thoughts to have) before forming a conclusion from the statement\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This statement lacks a clear stance or reasoning. Before forming a conclusion, it would be helpful to ask: What are the potential implications or reasoning behind the man's actions? Is there any context provided or reasoning given for his behavior?\n"
     ]
    }
   ],
   "source": [
    "statement = \"A Fargo, North Dakota, man was arrested for clearing snow with a flamethrower.\"\n",
    "\n",
    "detect_argument(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning atomizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file ID: file-3kgciCTKZWzLX3GrQ8hxzF\n",
      "Fine-tuning job started with ID: ftjob-lN0ZXS21LfRw1QhFXkO92Ndu\n",
      "Fine-tuning status: validating_files\n",
      "Fine-tuning status: validating_files\n",
      "Fine-tuning status: validating_files\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: succeeded\n",
      "Fine-tuning finished with status: succeeded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "atomizer_job_id = fine_tune_model(\"atomization_critical_analysis_finetuning_50.jsonl\")\n",
    "check_fine_tune_status(atomizer_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning status: succeeded\n"
     ]
    }
   ],
   "source": [
    "finetuned_models = []\n",
    "job_status = openai.FineTuningJob.retrieve(atomizer_job_id)\n",
    "print(f\"Fine-tuning status: {job_status['status']}\")\n",
    "\n",
    "if job_status == 'succeeded':\n",
    "    atomizer_model_name = get_finetuned_model_name(atomizer_job_id)\n",
    "\n",
    "    finetuned_models.append({'atomizer_model_name': atomizer_model_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:personal::B3yyOMLV\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-0125:personal::B3yyOMLV'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fine_tuned_model():\n",
    "    \"\"\"\n",
    "    Retrieve the fine-tuned model ID.\n",
    "    \"\"\"\n",
    "    fine_tuned_models = openai.FineTuningJob.list()\n",
    "    for job in fine_tuned_models[\"data\"]:\n",
    "        if job[\"status\"] == \"succeeded\":\n",
    "            print(f\"Fine-tuned Model ID: {job['fine_tuned_model']}\")\n",
    "            return job['fine_tuned_model']\n",
    "    \n",
    "    print(\"No fine-tuned model found.\")\n",
    "    return None\n",
    "\n",
    "# Example Usage:\n",
    "fine_tuned_model_id = get_fine_tuned_model()\n",
    "fine_tuned_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Name: ft:gpt-3.5-turbo-0125:personal::B3xAsgP1\n"
     ]
    }
   ],
   "source": [
    "argument_finetuned_model = get_finetuned_model_name('ftjob-zO22PDdApQaGp01RUlGhRvyL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Name: ft:gpt-3.5-turbo-0125:personal::B3yyOMLV\n"
     ]
    }
   ],
   "source": [
    "atomizer_finetuned_model = get_finetuned_model_name(atomizer_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_fine_tune_status(job_id):\n",
    "    while True:\n",
    "        job_status = openai.FineTuningJob.retrieve(job_id)\n",
    "        status = job_status[\"status\"]\n",
    "        print(f\"Fine-tuning status: {status}\")\n",
    "        \n",
    "        if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "            print(f\"Fine-tuning finished with status: {status}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(30)  # Check every 30 seconds\n",
    "\n",
    "# Replace with your actual job ID\n",
    "check_fine_tune_status(job_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_models = []\n",
    "\n",
    "finetuned_models.append({'argument_finetuned_model': argument_finetuned_model})\n",
    "finetuned_models.append({'atomizer_finetuned_model': atomizer_finetuned_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file ID: file-Y5qXqRKZyiNRteESBTWiT8\n",
      "Fine-tuning job started with ID: ftjob-AqtIXTDsomBgtA02llDOF5gL\n",
      "Fine-tuning status: validating_files\n",
      "Fine-tuning status: validating_files\n",
      "Fine-tuning status: validating_files\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: running\n",
      "Fine-tuning status: succeeded\n",
      "Fine-tuning finished with status: succeeded\n"
     ]
    }
   ],
   "source": [
    "critical_fine_tuned_model_id = fine_tune_model(\"critical_analysis_finetuning_50.jsonl\")\n",
    "check_fine_tune_status(critical_fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Name: ft:gpt-3.5-turbo-0125:personal::B3zr6iGR\n"
     ]
    }
   ],
   "source": [
    "critical_fine_tuned_model = get_finetuned_model_name(critical_fine_tuned_model_id)\n",
    "finetuned_models.append({'critical_fine_tuned_model': critical_fine_tuned_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'argument_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3xAsgP1'},\n",
       " {'atomizer_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3yyOMLV'},\n",
       " {'critical_fine_tuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3zr6iGR'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automating the end to end processing of input para to generate critical line of thoughts, to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'argument_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3xAsgP1'},\n",
       " {'atomizer_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3yyOMLV'},\n",
       " {'critical_fine_tuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3zr6iGR'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_text': 'Artificial intelligence will replace most jobs, but it will also create new opportunities in various sectors.', 'critical_analysis': [{'claim': '- A structured list of extracted claims', 'critique': 'Critical Assessment: The provided atomic claim - A structured list of extracted claims assumes This house would defund the police functions in a certain way, but it is essential to consider alternative perspectives. While A structured list of extracted claims, opposing research suggests that This house would defund the police could have different impacts under varying conditions. This house would defund the police-specific policies might be context-dependent, influenced by factors like A structured list of extracted claims-specific socioeconomic backgrounds, This house would defund the police This house would defund the police, or external variables. Therefore, this critical assessment highlights the need to consider a diverse range of factors when evaluating The provided atomic claim - A structured list of extracted claims, contextualizing its potential effects within specific This house would defund the police settings.assumes-functions in a certain way, but it is essential to consider alternative perspectives. While, opposing research suggests thatcould have different impacts under varying conditions.specific policies might be context-dependent, influenced by factors like-specific socioeconomic backgrounds,, or external variables. Therefore, this critical assessment highlights the need to consider a diverse range of factors when evaluating, contextualizing its potential effects within specificsettings.'}, {'claim': '- Artificial intelligence will replace most jobs.  ', 'critique': 'Critical Assessment:\\n\\nThe atomic claim \"Artificial intelligence will replace most jobs\" assumes This house believes that the House of Saud is a greater threat to peace than to ISIS functions in a certain way, but it is essential to consider alternative perspectives. While Artificial intelligence will replace most jobs, opposing research suggests that This house believes that the House of Saud is a greater threat to peace than to ISIS could have different impacts under varying conditions. Thus, it is possible that Artificial intelligence will replace most jobs\\'s validity is context-dependent, influenced by factors like This house believes that the House of Saud is a greater threat to peace than to ISIS-specific policies, socioeconomic backgrounds, or This house believes that the House of Saud is a greater threat to peace than to ISIS governance structures.\\n\\nHave you considered the possibility that Artificial intelligence will replace most jobs might be context-specific, with its impacts varying across This house believes that the House of Saud is a greater threat to peace than to ISIS differentially? While the existing literature on Artificial intelligence will replace most jobs has mainly focused on This house believes that the House of Saud is a greater threat to peace than to ISIS, it is essential to acknowledge that alternative mechanisms could come into play. For example, the effect of Artificial intelligence will replace most jobs might be moderated by external variables like This house believes that the House of Saud is a greater threat to peace than to ISIS-specific economic policies, geopolitical environments, or social frameworks, leading to diverse outcomes under different conditions.\\n\\nCritical assessment of the atomic claim \"Artificial intelligence will replace most jobs\":\\n\\nIn conclusion, the assumption that Artificial intelligence will replace most jobs functions in a certain way is not set in stone, leaving room for alternative interpretations and opposing views. While existing research has primarily focused on This house believes that the House of Saud is a greater threat to peace than to ISIS, it is crucial to consider the possibility that Artificial intelligence will replace most jobs\\'s effects could be context-dependent, influenced by a myriad of This house believes that the House of Saud is a greater threat to peace than to ISIS-specific factors. By critically examining the underlying assumptions of Artificial intelligence will replace most jobs, we can better understand the scope of its generalizability and pinpoint specific conditions under which This house believes that the House of Saud is a greater threat to peace than to ISIS is more likely to have certain impacts.'}, {'claim': '- Artificial intelligence functions autonomously and carries out tasks with human-like intelligence.  ', 'critique': \"Critical Assessment: The atomic claim 'Artificial intelligence functions autonomously and carries out tasks with human-like intelligence' assumes This house would ban the use of artificial intelligence in autonomous weapons functions autonomously and carries out tasks with human-like intelligence functions autonomously and carries out tasks with human-like intelligence in a certain way. Alternative perspectives might consider This house would ban the use of artificial intelligence in autonomous weapons functions autonomously and carries out tasks with human-like intelligence functions autonomously and carries out tasks with human-like intelligence in different contexts or under different conditions. While Artificial intelligence functions autonomously and carries out tasks with human-like intelligence This house would ban the use of artificial intelligence in autonomous weapons functions autonomously and carries out tasks with human-like intelligence, it is essential to recognize that opposing views exist. For example, proponents of Artificial intelligence functions autonomously and carries out tasks with human-like intelligence This house would ban the use of artificial intelligence in autonomous weapons functions autonomously and carries out tasks with human-like intelligence emphasize This house would ban the use of artificial intelligence in autonomous weapons functions autonomously and carries out tasks with human-like intelligence's positive aspects, while critics highlight its potential drawbacks. By critically assessing Artificial intelligence functions autonomously and carries out tasks with human-like intelligence This house would ban the use of artificial intelligence in autonomous weapons functions autonomously and carries out tasks with human-like intelligence and considering opposing viewpoints, we can gain a more holistic understanding of the subject.\"}, {'claim': '- Replace most jobs  ', 'critique': \"Critical Assessment: The atomic claim 'Replace most jobs' assumes This house supports the replacement of humans by machines functions in a certain way, but it is essential to consider alternative perspectives. While Replace most jobs, opposing research suggests that This house supports the replacement of humans by machines could have different impacts under varying conditions. Considering the context-dependent nature of Replace most jobs, it is possible that This house supports the replacement of humans by machines might be influenced by factors like Replace most jobs-specific policies, socioeconomic backgrounds, or external variables. Therefore, this critical assessment recognizes that the effects of Replace most jobs are not necessarily generalizable across different settings, and future research could explore how This house supports the replacement of humans by machines may interact with other variables in complex ways. Have you considered the possibility that Replace most jobs might be context-dependent, influenced by factors like This house supports the replacement of humans by machines-specific policies, socioeconomic backgrounds, or external variables? The atomic claim 'Replace most jobs' could be situated within a broader discursive context that assumes a specific relationship between Replace most jobs and This house supports the replacement of humans by machines. While existing research has primarily focused on This house supports the replacement of humans by machines, alternative frameworks could question the underlying assumptions of Replace most jobs and propose different conceptualizations of how This house supports the replacement of humans by machines functions. For example, critical perspectives on Replace most jobs might highlight its embeddedness in broader power structures, emphasizing This house supports the replacement of humans by machines's role in reinforcing existing inequalities or serving the interests of specific groups. By situating the atomic claim 'Replace most jobs' within a complex web of intersecting factors, this critical assessment underscores the multidimensionality of This house supports the replacement of humans by machines and the limitations of reductionist approaches to understanding Replace most jobs. Functions This house supports the replacement of humans by machines Assumes This house supports the replacement of humans by machines Functions This house supports the replacement of humans by machines Assumes This house supports the replacement of humans by machines Functions This house supports the replacement of humans by machines Assumes This house supports the replacement of humans by machines Functions This house supports the replacement of humans by machines Assumes This house supports the replacement of humans by machines Functions This house supports the replacement of humans by machines Assumes This house supports the replacement of humans by machines Functions This house supports the replacement of humans by machines Assumes This house supports the replacement of humans by machines Critical Assessment: The atomic claim 'Replace most jobs' assumes This house supports the replacement of humans by machines functions in a certain way, but it is essential to consider alternative perspectives. While Replace most jobs, opposing research suggests that This house supports the replacement of humans by machines could have different impacts under varying conditions. Considering the context-dependent nature of Replace most jobs, it is possible that This house supports the replacement of humans by machines might be influenced by factors like Replace most jobs-specific policies, socioeconomic backgrounds, or external variables. Therefore, this critical assessment recognizes that the effects of Replace most jobs are not necessarily generalizable across different settings, and future research could explore how This house supports the replacement of humans by machines may interact with other variables in complex ways. Have you considered the possibility that Replace most jobs might be context-dependent, influenced by factors like This house supports the replacement of humans by machines-specific policies, socioeconomic backgrounds, or external variables? The atomic claim 'Replace most jobs' could be situated within a broader discursive context that assumes a specific relationship between Replace most jobs and This house supports the replacement of humans by machines. While existing research has primarily focused on This house supports the replacement of humans by machines, alternative frameworks could question the underlying assumptions of Replace most jobs and propose different conceptualizations of how This house supports the replacement of humans by machines functions. For example, critical perspectives on Replace most jobs might highlight its embeddedness in broader power structures, emphasizing This house supports the replacement of humans by machines's role in reinforcing existing inequalities or serving the interests of specific groups. By situating the atomic claim 'Replace most jobs' within a complex web of intersecting factors, this critical assessment underscores the multidimensionality of This house supports the replacement of humans by machines and the limitations of reductionist approaches to understanding Replace most jobs.\"}, {'claim': '- Artificial intelligence will create new opportunities in various sectors.', 'critique': \"Critical Assessment:\\n\\nThe atomic claim 'Artificial intelligence will create new opportunities in various sectors' assumes This house believes that the production and sale of robots should be banned functions in a certain way, but it is essential to consider alternative perspectives. While Artificial intelligence will create new opportunities in various sectors, opposing research suggests that This house believes that the production and sale of robots should be banned could have different impacts under varying conditions. Therefore, it is crucial to question the underlying assumptions of Artificial intelligence will create new opportunities in various sectors, especially in the context of This house believes that the production and sale of robots should be banned-specific policies and socioeconomic backgrounds.\\n\\nAlternative Perspectives:\\n\\nAn opposing view could be that Artificial intelligence will create new opportunities in various sectors is context-dependent, influenced by factors like This house believes that the production and sale of robots should be banned-specific regulations, economic policies, or social frameworks. While Artificial intelligence will create new opportunities in various sectors under certain conditions, it is essential to consider the possibility that This house believes that the production and sale of robots should be banned might have different impacts in other contexts. For example, research on This house believes that the production and sale of robots should be banned-based interventions could provide valuable insights into the nuanced effects of Artificial intelligence will create new opportunities in various sectors across different populations.\\n\\nCritical Questions:\\n\\nWhat are the underlying assumptions of Artificial intelligence will create new opportunities in various sectors, and how valid are they in the context of This house believes that the production and sale of robots should be banned-specific research? Have alternative theories challenged the traditional understanding of Artificial intelligence will create new opportunities in various sectors, and if so, how do they address the limitations of existing studies? Can we identify and mitigate potential biases in the evidence base of Artificial intelligence will create new opportunities in various sectors, especially those derived from This house believes that the production and sale of robots should be banned-specific sources or disciplinary backgrounds? What have been the primary critiques of Artificial intelligence will create new opportunities in various sectors in the context of This house believes that the production and sale of robots should be banned, and how have different schools of thought responded to these criticisms?\\n\\nBiases to Consider:\\n\\nThere may be a publication bias towards positive results supporting Artificial intelligence will create new opportunities in various sectors, with studies that This house believes that the production and sale of robots should be banned-specific variables showing significant effects being more likely to get published and highlighted. Researchers in the This house believes that the production and sale of robots should be banned field could have ingrained assumptions about Artificial intelligence will create new opportunities in various sectors, affecting their study design and interpretation of results. Funding sources for This house believes that the production and sale of robots should be banned-related research might have influenced the framing of the existing evidence base, leading to potential conflicts of interest in the domain of Artificial intelligence will create new opportunities in various sectors. The use of specific outcome measures in This house believes that the production and sale of robots should be banned studies could have shaped the definition and operationalization of key variables in research on Artificial intelligence will create new opportunities in various sectors, potentially limiting the generalizability of findings across different contexts.è®®Atomic Claim: Artificial intelligence will create new opportunities in various sectors.\\nAlternative Perspectives: An opposing view could be that Artificial intelligence will create new opportunities in various sectors is context-dependent, influenced by factors like This house believes that the production and sale of robots should be banned-specific regulations, economic policies, or social frameworks. While Artificial intelligence will create new opportunities in various sectors under certain conditions, it is essential to consider the possibility that This house believes that the production and sale of robots should be banned might have different impacts in other contexts. For example, research on This house believes that the production and sale of robots should be banned-based interventions could provide valuable insights into the nuanced effects of Artificial intelligence will create new opportunities in various sectors across different populations.\\nCritical Questions: What are the underlying assumptions of Artificial intelligence will create new opportunities in various sectors, and how valid are they in the context of This house believes that the production and sale of robots should be banned-specific research? Have alternative theories challenged the traditional understanding of Artificial intelligence will create new opportunities in various sectors, and if so, how do they address the limitations of existing studies? Can we identify and mitigate potential biases in the evidence base of Artificial intelligence will create new opportunities in various sectors, especially those derived from This house believes that the production and sale of robots should be banned-specific sources or disciplinary backgrounds? What have been the primary critiques of Artificial intelligence will create new opportunities in various sectors in the context of This house believes that the production and sale of robots should be banned, and how have different schools of thought responded to these criticisms?\\nBiases to Consider: There may be a publication bias towards positive results supporting Artificial intelligence will create new opportunities in various sectors, with studies that This house believes that the production and sale of robots should be banned-specific variables showing significant effects being more likely to get published and highlighted. Researchers in the This house believes that the production and sale of robots should be banned field could have ingrained assumptions about Artificial intelligence will create new opportunities in various sectors, affecting their study design and interpretation of results. Funding sources for This house believes that the production and sale of robots should be banned-related research might have influenced the framing of the existing evidence base, leading to potential conflicts of interest in the domain of Artificial intelligence will create new opportunities in various sectors. The use of specific outcome measures in This house believes that the production and sale of robots should be banned studies could have shaped the definition and operationalization of key variables in research on Artificial intelligence will create new opportunities in various sectors, potentially limiting the generalizability of findings across different contexts.\"}, {'claim': '- Artificial intelligence will create new opportunities in various sectors.', 'critique': \"Critical Assessment: The atomic claim 'Artificial intelligence will create new opportunities in various sectors' assumes This house as the campus have students who are from economically deprived backgrounds  functions in a certain way, but alternative perspectives suggest that Artificial intelligence will create new opportunities in various sectors could have different impacts under varying conditions. While Artificial intelligence will create new opportunities in various sectors might be beneficial in some contexts, it is essential to consider the possibility that This house as the campus have students who are from economically deprived backgrounds-dependent factors could influence the outcomes. This house as the campus have students who are from economically deprived backgrounds-specific research on Artificial intelligence will create new opportunities in various sectors is still limited, so approaching the issue from different angles can shed light on potential biases and underlying assumptions. Have you considered the possibility that Artificial intelligence will create new opportunities in various sectors might interact with other variables in a non-linear way, leading to different outcomes under varying conditions?\"}]}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = config['openai_key']\n",
    "\n",
    "# Fine-tuned model names\n",
    "argument_finetuned_model = finetuned_models[0]['argument_finetuned_model']\n",
    "atomizer_finetuned_model = finetuned_models[1]['atomizer_finetuned_model']\n",
    "critical_fine_tuned_model = finetuned_models[2]['critical_fine_tuned_model']\n",
    "\n",
    "def analyze_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    End-to-end function that:\n",
    "    1. Detects if an argument/strong stance exists.\n",
    "    2. Extracts claims and atomizes them.\n",
    "    3. Generates critical perspectives for each atomic claim.\n",
    "    Returns structured critical thoughts.\n",
    "    \"\"\"\n",
    "\n",
    "    ### ðŸ”¹ Step 1: Detect Argument in Paragraph ###\n",
    "    argument_detection_prompt = f\"\"\"\n",
    "    Analyze the given paragraph and determine whether it contains a strong argument, claim, or stance.\n",
    "    If the paragraph contains an argument, extract the key claims or positions stated.\n",
    "    Output should be either:\n",
    "    None - if no arguments found in paragraph, if present, return the lines in the paragraph that contains the arguments. \n",
    "    Use the paragraph below to infer the arguments if any.\n",
    "    Paragraph: {paragraph}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=argument_finetuned_model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an AI that detects arguments and extracts claims from text.\"},\n",
    "                  {\"role\": \"user\", \"content\": argument_detection_prompt}]\n",
    "    )\n",
    "    \n",
    "    argument_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    if \"No Argument Found\" in argument_output:\n",
    "        return {\"message\": \"No argument detected in the paragraph.\"}\n",
    "\n",
    "    extracted_claims = argument_output.split(\"\\n\")  # Assuming claims are line-separated\n",
    "\n",
    "    ### ðŸ”¹ Step 2: Atomize the Extracted Claims ###\n",
    "    atomized_claims = []\n",
    "    \n",
    "    for claim in extracted_claims:\n",
    "        atomization_prompt = f\"\"\"\n",
    "        Break down the following claim into atomic facts.\n",
    "        Ensure each fact is self-contained, concise, and retains the original meaning.\n",
    "        Output should be a structured list of atomic claims.\n",
    "\n",
    "        Claim: {claim}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=atomizer_finetuned_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an AI that breaks down complex claims into atomic claims.\"},\n",
    "                      {\"role\": \"user\", \"content\": atomization_prompt}]\n",
    "        )\n",
    "        \n",
    "        atomized_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        atomized_claims.extend(atomized_output.split(\"\\n\"))  # Collecting all atomic claims\n",
    "\n",
    "    ### ðŸ”¹ Step 3: Generate Critical Perspectives ###\n",
    "    critical_thoughts = []\n",
    "\n",
    "    for atomic_claim in atomized_claims:\n",
    "        critique_prompt = f\"\"\"\n",
    "        Critically analyze the following atomic claim.\n",
    "        Provide alternative perspectives, question underlying assumptions, and explore potential biases.\n",
    "        Generate a structured critical assessment.\n",
    "\n",
    "        Atomic Claim: {atomic_claim}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=critical_fine_tuned_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an AI that generates critical perspectives on claims.\"},\n",
    "                      {\"role\": \"user\", \"content\": critique_prompt}]\n",
    "        )\n",
    "        \n",
    "        critique_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        critical_thoughts.append({\"claim\": atomic_claim, \"critique\": critique_output})\n",
    "\n",
    "    return {\"original_text\": paragraph, \"critical_analysis\": critical_thoughts}\n",
    "\n",
    "# Example Usage:\n",
    "paragraph = \"Artificial intelligence will replace most jobs, but it will also create new opportunities in various sectors.\"\n",
    "result = analyze_paragraph(paragraph)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'argument_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3xAsgP1'},\n",
       " {'atomizer_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3yyOMLV'},\n",
       " {'critical_fine_tuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3zr6iGR'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_models = {'argument_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3xAsgP1', \n",
    "                    'atomizer_finetuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3yyOMLV',\n",
    "                    'critical_fine_tuned_model': 'ft:gpt-3.5-turbo-0125:personal::B3zr6iGR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = config['openai_key']\n",
    "\n",
    "# Fine-tuned model names\n",
    "argument_finetuned_model = finetuned_models[0]['argument_finetuned_model']\n",
    "atomizer_finetuned_model = finetuned_models[1]['atomizer_finetuned_model']\n",
    "critical_fine_tuned_model = finetuned_models[2]['critical_fine_tuned_model']\n",
    "\n",
    "def analyze_paragraph(paragraph):\n",
    "    \"\"\"\n",
    "    End-to-end function that:\n",
    "    1. Detects if an argument/strong stance exists.\n",
    "    2. Extracts claims and atomizes them.\n",
    "    3. Generates critical perspectives for each atomic claim.\n",
    "    Returns structured critical thoughts.\n",
    "    \"\"\"\n",
    "\n",
    "    # ### ðŸ”¹ Step 1: Detect Argument in Paragraph ###\n",
    "    # argument_detection_prompt = f\"\"\"\n",
    "    # Analyze the given paragraph and determine whether it contains a strong argument, claim, or stance.\n",
    "    # If the paragraph contains an argument, extract the key claims or positions stated.\n",
    "    # Output should be either:\n",
    "    # if no arguments found in paragraph - return None\n",
    "    # else, only return the lines in the paragraph that contains the arguments. \n",
    "    # For example,\n",
    "    # Input: 'So if you think that people get value off of art or if people get value from listening to music, if it makes our lives better it's important, make sure the you're supporting the people who give you that sense of happiness when you listen to a great song and debate we which this is supported, is by making sure they're able to make a living by doing what they're doing, but without an intellectual property right it's, impossible for them to actually make this living because anyone can just rip them off use their song'\n",
    "    # Output: 'It argues why we should not abolish intellectual property rights'\"\n",
    "\n",
    "    # Using the above as an example, now infer the arguments if any present in the paragraph given by user.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=argument_finetuned_model,\n",
    "    #     messages=[{\"role\": \"system\", \"content\": argument_detection_prompt},\n",
    "    #               {\"role\": \"user\", \"content\": paragraph}]\n",
    "    # )\n",
    "    \n",
    "    # argument_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # # if \"No Argument Found\" in argument_output:\n",
    "    # #     return {\"message\": \"No argument detected in the paragraph.\"}\n",
    "\n",
    "    # extracted_claims = argument_output.split(\"\\n\")  # Assuming claims are line-separated\n",
    "    extracted_claims = paragraph.split('\\n')\n",
    "\n",
    "    ### ðŸ”¹ Step 2: Atomize the Extracted Claims ###\n",
    "    atomized_claims = []\n",
    "    # print(f\"argument model returns - {extracted_claims=}\")\n",
    "    for claim in extracted_claims:\n",
    "        atomization_prompt = f\"\"\"\n",
    "        Break down the following claim into atomic facts.\n",
    "        Ensure each fact is self-contained, concise, and retains the original meaning.\n",
    "        Output should be a structured list of atomic claims.\n",
    "\n",
    "        Claim: {claim}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=atomizer_finetuned_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an AI that breaks down complex claims into atomic claims.\"},\n",
    "                      {\"role\": \"user\", \"content\": atomization_prompt}]\n",
    "        )\n",
    "        \n",
    "        atomized_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        atomized_claims.extend(atomized_output.split(\"\\n\"))  # Collecting all atomic claims\n",
    "\n",
    "    ### ðŸ”¹ Step 3: Generate Critical Perspectives ###\n",
    "    critical_thoughts = []\n",
    "\n",
    "    for atomic_claim in atomized_claims:\n",
    "        critique_prompt = f\"\"\"\n",
    "        Critically analyze the following atomic claim.\n",
    "        Provide alternative perspectives, question underlying assumptions, and explore potential biases.\n",
    "        Generate a structured critical assessment.\n",
    "\n",
    "        Atomic Claim: {atomic_claim}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"system\", \"content\": \"\"\"Generate critical perspectives on claims given by user, challenge and expand claims by providing thought-provoking critiques. \n",
    "                       for example, \n",
    "                       Input: 'The one-child policy has been a major success, reducing overpopulation and improving China's economy. While controversial, most citizens support it, and studies show it has led to better healthcare for women. The government remains firm in its decision, arguing that any negative consequences are outweighed by long-term benefits.'\n",
    "                       Output: 'What are the ethical concerns about limiting family size? Is economic gain a justification for restricting personal freedoms?\\n- How reliable are the studies supporting this policy? Are they government-funded, and do they account for hidden consequences like gender imbalance?\\n- Are there long-term economic risks, such as an aging population and labor shortages, that counterbalance the short-term benefits?\\n- The claim that 'most citizens support it'\\u2014does this account for people who are afraid to speak against government policies?\\n- What alternative policies could have achieved the same outcome with less human rights impact?'\n",
    "                       Similar to above critical line of thought to evaluate the presented argument, generate critical lines of thought for user given input. \"\"\"},\n",
    "                      {\"role\": \"user\", \"content\": critique_prompt}]\n",
    "        )\n",
    "        \n",
    "        critique_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        critical_thoughts.append({\"claim\": atomic_claim, \"critique\": critique_output})\n",
    "    critique = '\\n'.join(ct['critique'] for ct in critical_thoughts)\n",
    "    critique = critique.strip(' ').strip('-').strip('\\n').strip(',')\n",
    "    # return {\"original_text\": paragraph, \"critical_analysis\": critical_thoughts}\n",
    "    return critique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "paragraph = \"Artificial intelligence will replace most jobs, but it will also create new opportunities in various sectors.\"\n",
    "result = analyze_paragraph(paragraph)\n",
    "print(result)   \n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze_content(content: str):\n",
    "    \"\"\"\n",
    "    API Endpoint to analyze a webpage's parsed content.\n",
    "    Expects a raw text string as input.\n",
    "    Returns critical thoughts as a response.\n",
    "    \"\"\"\n",
    "    if not content:\n",
    "        raise HTTPException(status_code=400, detail=\"Content cannot be empty.\")\n",
    "\n",
    "    critical_analysis = analyze_paragraph(content)\n",
    "    \n",
    "    return {\"critical_thoughts\": critical_analysis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def atomize_claims(paragraph):\n",
    "    \"\"\"\n",
    "    Breaks down a complex paragraph into atomic claims.\n",
    "    \"\"\"\n",
    "    # Splitting at conjunctions that indicate multiple ideas\n",
    "    atomic_claims = [c.strip() for c in re.split(r'\\band\\b|\\bbut\\b|\\bhowever\\b|\\bor\\b|\\bso\\b', paragraph) if c]\n",
    "    \n",
    "    return atomic_claims\n",
    "\n",
    "# Example Usage:\n",
    "text = \"The one-child policy helped reduce overpopulation, but it led to severe social issues like gender imbalance and aging population.\"\n",
    "atomic_claims = atomize_claims(text)\n",
    "print(atomic_claims)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def atomize_claims(paragraph):\n",
    "    \"\"\"\n",
    "    Breaks down a complex paragraph into atomic claims.\n",
    "    \"\"\"\n",
    "    # Splitting at conjunctions that indicate multiple ideas\n",
    "    atomic_claims = [c.strip() for c in re.split(r'\\band\\b|\\bbut\\b|\\bhowever\\b|\\bor\\b|\\bso\\b', paragraph) if c]\n",
    "    \n",
    "    return atomic_claims\n",
    "\n",
    "# Example Usage:\n",
    "text = \"The one-child policy helped reduce overpopulation, but it led to severe social issues like gender imbalance and aging population.\"\n",
    "atomic_claims = atomize_claims(text)\n",
    "print(atomic_claims)\n",
    "\n",
    "\n",
    "def set_atomizer_prompt(text):\n",
    "    \"\"\"\n",
    "    Generates the atomizer prompt for breaking down a complex statement.\n",
    "    \"\"\"\n",
    "    atomizer_prompt = \"\"\"\n",
    "    Break down the given statement into atomic facts. \n",
    "    Ensure each fact is self-contained and retains its logical meaning.\n",
    "    Output a structured list of independent claims.\n",
    "    \"\"\"\n",
    "    return f\"{atomizer_prompt}\\n\\nInput: {text}\\nOutput:\"\n",
    "\n",
    "# Example Usage:\n",
    "prompt = set_atomizer_prompt(text)\n",
    "print(prompt)\n",
    "\n",
    "def generate_critical_perspective(atomic_claims):\n",
    "    \"\"\"\n",
    "    Uses the fine-tuned critique model to generate critical perspectives for each atomic claim.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "\n",
    "    for claim in atomic_claims:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=critical_fine_tuned_model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a critical thinking AI that evaluates claims and provides critiques.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the following claim and provide a critical perspective: '{claim}'\"}\n",
    "            ]\n",
    "        )\n",
    "        critique = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        responses.append({\"claim\": claim, \"critique\": critique})\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Example Usage:\n",
    "if critical_fine_tuned_model_id:\n",
    "    critiques = generate_critical_perspective(atomic_claims, fine_tuned_model_id)\n",
    "    print(critiques)\n",
    "\n",
    "def analyze_text(text, fine_tuned_model_id):\n",
    "    \"\"\"\n",
    "    Complete workflow: \n",
    "    1. Atomize a paragraph into independent claims.\n",
    "    2. Generate a critique for each atomic claim.\n",
    "    \"\"\"\n",
    "    atomic_claims = atomize_claims(text)\n",
    "    critiques = generate_critical_perspective(atomic_claims)\n",
    "\n",
    "    return {\"original_text\": text, \"analysis\": critiques}\n",
    "\n",
    "# Example Usage:\n",
    "text = \"AI will completely replace human jobs in the future, but it will also create new opportunities in different sectors.\"\n",
    "result = analyze_text(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "choices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/unbias-me-pY5bl0c_-py3.10/lib/python3.10/site-packages/openai/openai_object.py:59\u001b[0m, in \u001b[0;36mOpenAIObject.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[k]\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'choices'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Generate a critique using the fine-tuned model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39margument_finetuned_model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCheck for arguments made in the given text (if any): \u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe earth is flat says the team of scientists\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/varshinibalaji/ds_projects/unbias_me/models/openai.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39;49mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip())\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/unbias-me-pY5bl0c_-py3.10/lib/python3.10/site-packages/openai/openai_object.py:61\u001b[0m, in \u001b[0;36mOpenAIObject.__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[k]\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m*\u001b[39merr\u001b[39m.\u001b[39margs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: choices"
     ]
    }
   ],
   "source": [
    "# Generate a critique using the fine-tuned model\n",
    "response = openai.ChatCompletion(\n",
    "    model=argument_finetuned_model,\n",
    "    prompt=\"Check for arguments made in the given text (if any): 'The earth is flat says the team of scientists'\",\n",
    "    max_tokens=150\n",
    ")\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unbias-me-pY5bl0c_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
